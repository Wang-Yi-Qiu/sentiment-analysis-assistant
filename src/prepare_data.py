import os
import sys
from transformers import AutoTokenizer
from config import Config
from dataset import DataProcessor

def main():
    print("â³ å¼€å§‹ä¸‹è½½å¹¶å¤„ç†æ•°æ®...")
    
    # 1. ç¡®ä¿ data ç›®å½•å­˜åœ¨
    if not os.path.exists(Config.DATA_DIR):
        os.makedirs(Config.DATA_DIR)
        
    # 2. åˆå§‹åŒ–æµç¨‹
    tokenizer = AutoTokenizer.from_pretrained(Config.BASE_MODEL)
    processor = DataProcessor(tokenizer)
    
    # 3. è·å–å¤„ç†åçš„æ•°æ® (get_processed_dataset å†…éƒ¨å·²ç»æœ‰åŠ è½½é€»è¾‘)
    # æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä¸ºäº†ä¿å­˜åŸå§‹æ•°æ®ï¼Œå¯èƒ½éœ€è¦è°ƒç”¨ load_clap_data å’Œ load_medical_data
    # ä½† DataProcessor.get_processed_dataset è¿”å›çš„æ˜¯ encode åçš„æ•°æ®ã€‚
    # ç”¨æˆ·å¯èƒ½æƒ³è¦çš„æ˜¯ Raw Data æˆ–è€… Processed Dataã€‚
    # è¿™é‡Œæˆ‘ä»¬ä¿å­˜ Processed Data (Ready for Training) åˆ°ç£ç›˜
    
    dataset = processor.get_processed_dataset()
    
    save_path = os.path.join(Config.DATA_DIR, "processed_dataset")
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å¤„ç†åçš„æ•°æ®é›†åˆ°: {save_path}")
    dataset.save_to_disk(save_path)
    
    print("âœ… æ•°æ®ä¿å­˜å®Œæˆï¼")
    print(f"   Train set size: {len(dataset['train'])}")
    print(f"   Test set size: {len(dataset['test'])}")
    print("   ä¸‹æ¬¡åŠ è½½å¯ç›´æ¥ä½¿ç”¨: from datasets import load_from_disk")

if __name__ == "__main__":
    main()
